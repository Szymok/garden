---
title: Deep Learning DDIM
type: course
tags: 
aliases:
---
- Weights and Biases (W&B) for experiment tracking
- Fréchet Inception Distance (FID) metric
- Kernel Inception Distance (KID) metric
- Denoising Diffusion Implicit Model (DDIM)

In the first section of the video lesson, Jeremy, Johno, and Tanishq discuss the progress of their experiments using the Fashion-MNIST dataset and the idea of moving to larger datasets and more difficult tasks. Johno introduces the CIFAR-10 dataset, a popular dataset for image classification and generative modeling. They discuss the challenges of visually inspecting the CIFAR-10 dataset due to its low-quality images.

Johno demonstrates how he used the same noisify function and UNet model with slight modifications for the CIFAR-10 dataset. He also introduces Weights and Biases (W&B), an experiment tracking and logging tool that can help manage and visualize the progress of their experiments. W&B allows users to log various metrics, save models as artifacts, and create reports for sharing results. Johno shows how he integrated W&B with the miniai library using a custom callback.

Jeremy and Tanishq discuss the benefits of using W&B for experiment tracking, such as collaboration, reproducibility, and convenience. However, Jeremy also emphasizes the importance of not relying solely on experiment tracking tools and focusing on carefully thought-out hypotheses and code changes. The discussion concludes with the acknowledgment that they will not be covering UNets in this lesson, but they have good reasons for deviating from their original plan.In this section of the video lesson, the instructor discusses new research directions and introduces the Fréchet Inception Distance (FID) metric to measure the quality of generated images. The FID metric is used to determine how similar generated images are to real images by comparing the means and covariance matrices of their features. The instructor demonstrates how to calculate the FID using a custom Fashion-MNIST model instead of the commonly used Inception model, as it is more accurate for the specific task of recognizing fashion.

The instructor explains the process of calculating the FID by first extracting features from a pre-trained model and then calculating the means and covariance matrices for both the real and generated images. The Fréchet Inception Distance is then calculated by comparing the similarity between the two covariance matrices and the two mean matrices. The instructor also discusses the Newton-Schurz method for calculating the matrix square root, which is used in the FID calculation.

The instructor highlights some caveats of the FID metric, such as its dependence on the number of samples used and the potential issues with resizing images when using the Inception model. Using a custom model trained on the specific data, like Fashion-MNIST in this case, can provide a more accurate and relevant FID metric for the task at hand.In this section of the video lesson, the presenter discusses the Fréchet Inception Distance (FID) and Kernel Inception Distance (KID) metrics for comparing image distributions. While FID is simple and automated, it has several caveats and biases. KID, on the other hand, is less biased but has high variance, making it less useful in practice. The presenter then introduces the ImageEval class for evaluating images using these metrics.

The presenter also shares an experience of fixing a bug in their code, which initially seemed to make the results worse. After further investigation and questioning the standard practices, they discovered that changing the input range of images from -1 to 1 to -0.5 to 0.5 improved the FID score. This led to questioning other standard practices, such as the linear schedule for noise addition, and experimenting with alternative schedules like the cosine schedule.

Ultimately, the presenter decides to use a linear schedule with a betamax of 0.01, which produces similar results to the cosine schedule. This allows them to keep their existing code mostly unchanged while still improving the performance of their model.In this section of the video lesson, the instructor discusses the improvements made to the DDPM_v2 model, resulting in the creation of Fashion DDPM_v3. The model's channels were doubled, and the number of epochs was increased by three, leading to better results. The Fréchet Inception Distance (FID) for the generated samples was nearly as good as real images, indicating high image quality for small, unconditional sampling.

The instructor then explores ways to make the model faster without sacrificing quality. By calling the model every third time and fine-tuning the last 50 iterations, the model becomes three times faster with only a slight increase in FID. The instructor also experiments with different schedules for how often the model is called, resulting in even faster sampling times.

The instructor introduces the Denoising Diffusion Implicit Model (DDIM) as a faster alternative to DDPM. The instructor demonstrates how to build a custom DDIM from scratch, using the existing implementation in the Diffusers library as a starting point. The DDIM approach allows for fewer steps in the sampling process while maintaining similar FID scores. The instructor concludes by discussing the benefits of using DDIM over DDPM, including faster sampling times and more concise code.In this section of the video lesson, the discussion focuses on the differences between DDPM and DDIM, as well as the benefits of using DDIM for rapid sampling. DDPM uses a fixed amount of noise, while DDIM introduces a parameter, sigma, which controls the amount of noise in the process. This allows for more control over the stochasticity of the model and even the possibility of making the process deterministic by setting sigma to zero.

The main advantage of DDIM is that it can be used with the same trained model as DDPM, making it a new sampling algorithm rather than a new training method. This is achieved by introducing a new parameter, eta, which controls the amount of noise in the process. When eta is set to one, it corresponds to regular DDPM, while setting it to zero results in a deterministic case.

Lastly, the discussion highlights the benefits of using DDIM for rapid sampling. By defining a similar distribution with a subset of diffusion steps, the same training objective can be met, allowing for faster sampling. This is particularly convenient with the cosine schedule, as it simplifies the code and allows for more flexibility with the eta parameter. The exploration of deterministic versus stochastic processes is an ongoing area of interest in this research.
