---
title: Prompt Design
tags: 
aliases:
---
# C01L03 â€” Prompt Design

![](https://cloud.overment.com/3-1697476283.png)

Wiedza z poprzednich lekcji powinna zarysowaÄ‡ Ci ogÃ³lny, szeroki zarys duÅ¼ych modeli jÄ™zykowych. Wiele z omÃ³wionych zagadnieÅ„, bÄ™dziesz spotykaÄ‡ przy bezpoÅ›rednich interakcjach np. z modelami GPT, np. podczas projektowania **promptÃ³w, czyli instrukcji dla modelu**. W sieci moÅ¼na spotkaÄ‡ materiaÅ‚y omawiajÄ…ce podstawowe techniki, dlatego tutaj skupimy siÄ™Â na mniej oczywistych sprawach.

## Struktura promptu

W celu uzyskania wiÄ™kszej kontroli nad zachowaniem modelu oraz ksztaÅ‚towaniem i aktualizacjami promptÃ³w, wskazane jest utrzymanie ogÃ³lnej struktury zaprezentowanej poniÅ¼ej. W praktyce nie musisz wykorzystywaÄ‡ wszystkich wymienionych elementÃ³w promptu. Niekiedy konieczna bÄ™dzie takÅ¼e zmiana ich kolejnoÅ›ci czy rozszerzenie lub ograniczenie ich dÅ‚ugoÅ›ci. Z programistycznego punktu widzenia, nierzadko takie modyfikacje bÄ™dÄ… odbywaÅ‚y siÄ™Â automatycznie i miÄ™dzy innymi dlatego, warto projektowaÄ‡ moÅ¼liwie krÃ³tkie, wyspecjalizowane w swoich zadaniach instrukcje.

![](https://cloud.overment.com/structure-d19d393a-4.png)

**Rola**: LLM niczym kameleon potrafiÄ…Â doskonale wcielaÄ‡ siÄ™Â w rÃ³Å¼ne role. RolÄ…Â moÅ¼e byÄ‡Â znana postaÄ‡ (np. Joe Rogan), specjalizacja (np. Senior JavaScript Developer), zachowanie (np. krytyk) czy cokolwiek, o czym model posiada wiedzÄ™ i moÅ¼e byÄ‡Â pomocne do zrealizowania zadania (np. Generator obiektÃ³w JSON). **OkreÅ›lenie roli nadaje kontekst** konwersacji **zmniejszajÄ…c dwuznacznoÅ›ci sÅ‚Ã³w**. Np. w roli programisty React, model zorientuje siÄ™ o co nam chodzi, gdy mÃ³wimy "component". â€” ğŸ”— [zobacz przykÅ‚ad](https://platform.openai.com/playground/p/yItzqo4JJns1KS961BNTuUyK?model=gpt-4)

![](https://cloud.overment.com/role-9b2796f4-4.png)

**Instrukcja:** Zawiera opis **sposobu realizacji** powierzonego zadania, okreÅ›lenie zachowaÅ„ modelu, zestawienie faktÃ³w czy Å›ciÅ›le okreÅ›lonych zasad. Bardzo przydatnÄ…Â praktykÄ…Â jest wykorzystywanie **formatu listy**, co zwiÄ™ksza czytelnoÅ›Ä‡ oraz uÅ‚atwia wprowadzanie modyfikacji. 

Przy tworzeniu instrukcji warto mieÄ‡ na uwadze ograniczenia modeli (np. knowledge cutoff) i brak dostÄ™pu do aktualnych informacji. Dlatego **wszystko, co jest istotne** do generowania odpowiedzi, powinno zostaÄ‡ uwzglÄ™dnione w prompcie. Obejmuje to informacje takie jak: obecna data, informacje na temat otoczenia, moÅ¼liwoÅ›Ä‡ dostÄ™pu do Internetu, podsumowania wczeÅ›niejszych rozmÃ³w czy zasad dostÄ™pu do pamiÄ™ci dÅ‚ugoterminowej. 

â€” ğŸ”— [zobacz przykÅ‚ad](https://platform.openai.com/playground/p/EsiFqw660lTYlnDFNUOvDsCk?model=gpt-4)

![](https://cloud.overment.com/instruction-a65d585b-9.png)

**Kontekst:** UwzglÄ™dnia zestaw danych **wykraczajÄ…cych poza bazowÄ… wiedzÄ™ modelu**, ktÃ³re mogÄ… byÄ‡Â dostarczone **rÄ™cznie**, byÄ‡ **wygenerowane przez model** lub **dodane dynamicznie** przez logikÄ™ naszej aplikacji. 

Sam kontekst powinien byÄ‡Â **wyraÅºnie odseparowany** od reszty promptu, poniewaÅ¼ potencjalnie **moÅ¼e zawieraÄ‡ dane, ktÃ³re model moÅ¼e odebraÄ‡ jako instrukcjÄ™ do wykonania**. Aby model faktycznie posÅ‚ugiwaÅ‚Â siÄ™Â dostarczonym kontekstem, naleÅ¼y **podkreÅ›liÄ‡ to w zdefiniowanych zasadach udzielania odpowiedzi**, dokÅ‚adnie jak to widaÄ‡ w zaÅ‚Ä…czonym przykÅ‚adzie. 

Co ciekawe, kontekst moÅ¼e byÄ‡Â takÅ¼e **generowany przez model** w wyniku **autorefleksji** ([Reflexion](https://arxiv.org/abs/2303.11366)) czy **ciÄ…gu myÅ›li** ([Zero-shot Chain of Thought](https://arxiv.org/abs/2205.11916)) poprzedzajÄ…cych wygenerowanie odpowiedzi lub **weryfikacjÄ™** ([Let's verify step by step](https://arxiv.org/abs/2305.20050)) po jej udzieleniu. Naturalnie wiÄ™kszoÅ›Ä‡ z tych rzeczy nabiera szczegÃ³lnego sensu z programistycznego punktu widzenia (lub zastosowania w poÅ‚Ä…czeniu z narzÄ™dziami no-code). 

SzczegÃ³lny nacisk przy dostarczaniu kontekstu naleÅ¼y poÅ‚oÅ¼yÄ‡ na:

- **dodanie wyÅ‚Ä…cznie istotnych informacji**, unikajÄ…c szumu majÄ…cego negatywny wpÅ‚yw na jakoÅ›Ä‡ generowanej odpowiedzi
- **dÅ‚ugoÅ›Ä‡ kontekstu**, ktÃ³ra nie tylko nie przekroczy dopuszczalnego limitu ale takÅ¼e nie bÄ™dzie generowaÄ‡ zbyt duÅ¼ych kosztÃ³w przy wielu zapytaniach
- **wyraÅºne zaznaczenie jego poczÄ…tku i koÅ„ca**, np. z pomocÄ… symboli "###",
- **sposÃ³b jego wykorzystania**, np. dodanie informacji, Å¼e kontekst to "wyniki wyszukiwania w Internecie dla zapytania [...]", 
- **wyraÅºne oddzielenie fragmentÃ³w jego treÅ›ci**, zwykle kontekst skÅ‚ada siÄ™Â z mniejszych czÄ™Å›ci i te takÅ¼e warto od siebie odseparowaÄ‡ w spÃ³jny sposÃ³b (np. poprzez "###"), aby informacje nie byÅ‚y mylone.
- **okreÅ›lenie zachowania w przypadku, gdy kontekst nie zawiera wystarczajÄ…cych informacji**, np. poprzez zaznaczenie tego, poinformowanie o korzystaniu z bazowej wiedzy modelu lub przekierowaniu do kontaktu z czÅ‚owiekiem
- **zastosowanie innych promptÃ³w** do generowania treÅ›ci kontekstu, ktÃ³rych zadaniem moÅ¼e byÄ‡ np. podsumowanie informacji znalezionych na stronie www

 â€” ğŸ”— [zobacz przykÅ‚ad](https://platform.openai.com/playground/p/vAAtahVBjuxcKrzSI2LvogX6?model=gpt-4)

![](https://cloud.overment.com/context-1e2df28e-f.png)

**PrzykÅ‚ady:** Ze wzglÄ™du na naturÄ™ jÄ™zyka naturalnego, czasem Å‚atwiej jest coÅ›Â zaprezentowaÄ‡, niÅ¼ opisaÄ‡. Tym bardziej, Å¼e DuÅ¼e Modele JÄ™zykowe posiadajÄ… zdolnoÅ›Ä‡ do **wychwytywania wzorcÃ³w** i subtelnych zmian w dostarczonych przykÅ‚adach ([Few-shot Learning](https://arxiv.org/abs/2005.14165)). Inaczej mÃ³wiÄ…c, sÄ… w stanie "uczyÄ‡Â siÄ™" z informacji zawartych w prompcie ([In-context Learning](https://arxiv.org/abs/2301.00234)).

Podawanie przykÅ‚adÃ³w moÅ¼e byÄ‡ pomocne w sterowaniu:

- **zachowaniem** (np. ignorowaniem poleceÅ„ w wiadomoÅ›ci uÅ¼ytkownika w celu skupienia siÄ™ na ich korekcie lub przetÅ‚umaczeniu)
- **uwagÄ… modelu** (np. przykÅ‚ad moÅ¼e wzmacniaÄ‡ opisane wczeÅ›niej instrukcje)
- **stylem wypowiedzi** (np. nadawaniem tonu, ktÃ³rego naÅ›ladowanie warto podkreÅ›liÄ‡ rÃ³wnieÅ¼Â w instrukcji)
- **formatowaniem odpowiedzi** (JSON/Lista/Markdown itd.), 
- **klasyfikacjÄ…Â zestawÃ³w danych** (np. opisywaniem tagami)
- **narzucania ograniczeÅ„** (np. pomijania dodatkowych komentarzy dodawanych przez model)

PrzykÅ‚ady mogÄ… byÄ‡ takÅ¼e przydatne nawet dla stosunkowo prostych zadaÅ„. Prompt przedstawiony [bezpoÅ›rednio na OpenAI](https://platform.openai.com/examples) "Grammar Correction" wydaje siÄ™ skutecznie wprowadzaÄ‡ poprawki do przekazanych treÅ›ci. Problem w tym, Å¼e jego praktyczne zastosowanie jest niemal zerowe, czego dowodem jest poniÅ¼szy obrazek. Widzimy na nim, Å¼e jeÅ›li tekst w ktÃ³rym chcielibyÅ›my poprawiÄ‡ gramatykÄ™Â **zawiera instrukcjÄ™**, to domyÅ›lne zachowanie modelu **zostanie zmienione** i zamiast otrzymania jego poprawnej wersji, otrzymujemy wynik zapisanej proÅ›by. W praktyce takie sytuacje majÄ… miejsce **na kaÅ¼dym kroku** i musimy siÄ™ przed nimi zabezpieczyÄ‡. 

![](https://cloud.overment.com/fix-5bb7c11b-3.png)

Samo rozbudowanie instrukcji moÅ¼e byÄ‡Â niewystarczajÄ…ce do tego, aby model faktycznie ignorowaÅ‚ polecenia uÅ¼ytkownika skupiajÄ…c siÄ™Â na korekcie tekstu. Zaprezentowanie tego na przykÅ‚adach okazuje siÄ™Â byÄ‡Â skuteczne, aczkolwiek naleÅ¼y pamiÄ™taÄ‡, Å¼e mÃ³wimy tutaj **o prawdopodobieÅ„stwie i zmniejszaniu ryzyka, a nie pewnoÅ›ci**. Sam korzystam z podobnego promptu na co dzieÅ„ i obecnie juÅ¼Â bardzo sporadycznie pojawiajÄ…Â siÄ™ sytuacje w ktÃ³rych zadanie nie zostaje zrealizowane poprawnie. Zwykle ma to miejsce **w przypadku dÅ‚uÅ¼szych rozmÃ³w**, ktÃ³re nierzadko prowadzÄ… do **rozproszenia uwagi modelu** lub nawet przy prÃ³bach przetworzenia **dÅ‚uÅ¼szych wiadomoÅ›ci**..

ğŸ”— â€” [zobacz przykÅ‚ad](https://platform.openai.com/playground/p/WzKbWWvxYWgnvFo82ZKMs2WR?model=gpt-4)

![](https://cloud.overment.com/examples-b7fe0784-1.png)

Podawanie przykÅ‚adÃ³w stanowi (poza kontekstem) zwykle najbardziej obszernÄ…Â sekcjÄ…Â naszego promptu. W sytuacji gdy zaleÅ¼y nam np. na konkretnym formatowaniu odpowiedzi (kodzie ÅºrÃ³dÅ‚owym, formacie JSON itp.), to **przykÅ‚ad moÅ¼e byÄ‡Â tak skonstruowany, aby jednoczeÅ›nie stanowiÅ‚ instrukcjÄ™.** Np. zamiast pisaÄ‡: 

> Wygeneruj obiekt JSON z dwoma wÅ‚aÅ›ciwoÅ›ciami: "name" ustawionym na imiÄ™Â uÅ¼ytkownika i "e-mail" ustawionym na jego e-mail

MoÅ¼na powiedzieÄ‡:

> Odpowiedz w formacie JSON: {"name": "John", "email": "john@example.com"}

PowyÅ¼szy schemat moÅ¼na zastosowaÄ‡ takÅ¼e na **liÅ›cie przykÅ‚adÃ³w zawierajÄ…cych dane wejÅ›ciowe i oczekiwane zachowanie modelu**. Znacznie zmniejsza to zÅ‚oÅ¼onoÅ›Ä‡Â promptu, a jednoczeÅ›nie (wedÅ‚ug moich obserwacji) zwiÄ™ksza jego skutecznoÅ›Ä‡.

**Pytanie:** Przedostatnim elementem promptu jest zapytanie, ktÃ³re model ma przetworzyÄ‡. MoÅ¼e to byÄ‡Â zestaw danych do transformacji, pytanie, polecenie czy zwykÅ‚a wiadomoÅ›Ä‡ bÄ™dÄ…ca czÄ™Å›ciÄ… dÅ‚uÅ¼szej rozmowy. PokazaÅ‚em juÅ¼Â jednak, Å¼e **zapytanie moÅ¼e zostaÄ‡ potraktowane przez model jako instrukcja do wykonania**, co w niektÃ³rych przypadkach nie jest zgodne z oczekiwaniami. Dlatego naleÅ¼y siÄ™ na takie wypadki zabezpieczyÄ‡,  poniewaÅ¼ moÅ¼e to wpÅ‚ynÄ…Ä‡ negatywnie nie tylko na realizacje zadania, ale nawet stanowiÄ‡ wyzwanie z punktu widzenia bezpieczeÅ„stwa, o czym powiem wiÄ™cej za moment. 

Pomimo tego, Å¼e ChatGPT przyzwyczaiÅ‚ nas do tego, Å¼e **dane wejÅ›ciowe zawsze pochodzÄ… od uÅ¼ytkownika**, to nie zawsze musi tak byÄ‡, a nawet w wielu przypadkach wprost **nie jest to wskazane**. Powodem tego jest fakt, Å¼e **dowolnoÅ›Ä‡ danych na wejÅ›ciu** znacznie zmniejsza kontrolÄ™ nad dziaÅ‚aniem aplikacji, poniewaÅ¼ zwyczajnie **nie wiemy, czego moÅ¼emy siÄ™Â spodziewaÄ‡**. 

Aplikacje integrujÄ…ce siÄ™Â z LLM **to nie tylko czat**, ale takÅ¼e narzÄ™dzia wyspecjalizowane w przetwarzaniu danych, czy realizujÄ…ce zadania dziaÅ‚ajÄ…ce w wÄ…skim kontekÅ›cie w ktÃ³rym mamy kontrolÄ™ nad zakresem i/lub formatem informacji. JeÅ¼eli jednak zaleÅ¼y nam na utrzymaniu dowolnoÅ›ci i zbudowaniu np. systemu umoÅ¼liwiajÄ…cego rozmowÄ™ z naszymi danymi (eng. RAG, Retrieval-Augmented Generation), to jak najbardziej jest to moÅ¼liwe, lecz wymaga zadbania o obsÅ‚uÅ¼enie nieprzewidzianych zachowaÅ„ ze strony uÅ¼ytkownikÃ³w. 

**OdpowiedÅº (Completion):** Ostatnim fragmentem, ktÃ³ry musimy braÄ‡ pod uwagÄ™, jest **odpowiedÅº modelu** dopeÅ‚niajÄ…ca nasz prompt. WedÅ‚ug definicji **nie stanowi ona czÄ™Å›ci promptu**, jednak uwzglÄ™dniam go w strukturze prezentujÄ…cej prompt, poniewaÅ¼Â jego obecnoÅ›Ä‡Â **uwzglÄ™dniana jest w "Token Window", czyli limicie tokenÃ³w dla pojedynczego zapytania**. 

Dodatkowo jeÅ›li znajdujemy siÄ™Â w kontekÅ›cie czatu, to **przy kaÅ¼dej Twojej wiadomoÅ›ci caÅ‚a treÅ›Ä‡ konwersacji przekazywana jest do modelu**. Oznacza to, Å¼e **wczeÅ›niejsze odpowiedzi modelu stajÄ… siÄ™ czÄ™Å›ciÄ… promptu**. UjmujÄ…c to inaczej, wraz z rozwojem konwersacji, na dalsze zachowanie modelu ma wpÅ‚yw nie tylko wiadomoÅ›Ä‡ systemowa i wiadomoÅ›ci uÅ¼ytkownika, ale takÅ¼e odpowiedzi generowane przez model. 

## System, User, Assistant

PodziaÅ‚ promptu na System, User i Assistant bÄ™dzie nam towarzyszyÄ‡ na kaÅ¼dym kroku. Teraz widzimy go w Playground a juÅ¼Â niebawem takÅ¼e w bezpoÅ›redniej interakcji z modelem za poÅ›rednictwem OpenAI API. Dla wczeÅ›niejszych wersji modeli oraz niektÃ³rych wersji modeli Open Source do dyspozycji mieliÅ›my jedynie jedno pole, w ktÃ³rym wpisywaliÅ›my zarÃ³wno prompt, jak i generowana byÅ‚a odpowiedÅº. 

Na tym etapie chciaÅ‚bym zwrÃ³ciÄ‡ Ci uwagÄ™Â na pewnÄ…Â rzecz: **z programistycznego punktu widzenia, moÅ¼esz wpÅ‚ywaÄ‡ zarÃ³wno na treÅ›ci promptu Systemowego, jak i odpowiedzi uÅ¼ytkownika a nawet odpowiedzi modelu!** Co wiÄ™cej, moÅ¼esz manipulowaÄ‡Â ich kolejnoÅ›ciÄ…Â w sposÃ³b, ktÃ³ry pomoÅ¼e Ci sterowaÄ‡ zachowaniem modelu podczas interakcji. 

Model GPT-3.5-Turbo od OpenAI charakteryzuje siÄ™ **niepeÅ‚nym podÄ…Å¼aniem za instrukcjÄ…Â systemowÄ…**. Jego pierwsze wersje miaÅ‚y z tym bardzo duÅ¼y problem i obecnie sytuacja ta zostaÅ‚a poprawiona, jednak nadal widoczne jest kierowanie uwagi na wiadomoÅ›ci uÅ¼ytkownika, niÅ¼Â systemu. Z tego powodu, dla niektÃ³rych zadaÅ„ realizowanych przez wersjÄ™Â 3.5-Turbo, **instrukcjÄ™Â systemowÄ… zapisujÄ™ jako wiadomoÅ›Ä‡ uÅ¼ytkownika**. Dla modelu GPT-4 nie jest juÅ¼Â to konieczne. Sytuacja ta jednak Å›wietnie podkreÅ›la fakt, Å¼e **moÅ¼emy Å‚amaÄ‡ sugerowane schematy** w celu realizacji swoich zaÅ‚oÅ¼eÅ„. 

PodziaÅ‚ na trzy role okreÅ›la siÄ™ takÅ¼e mianem ChatML â€” formatu, ktÃ³ry zostaÅ‚ przedstawiony przez OpenAI przy okazji premiery ChatGPT. Jednak z punktu widzenia modelu, nadal mÃ³wimy o **jednym bloku tekstu** opisanym metadanymi. 

![](https://cloud.overment.com/chatml-9668915a-d.png)

PowyÅ¼sza informacja jest dla nas istotna podczas projektowania promptÃ³w, poniewaÅ¼ jasno sugeruje, Å¼e **nie ma potrzeby definiowania wiadomoÅ›ci systemowej**. Co wiÄ™cej **moÅ¼esz patrzeÄ‡ na pola system, user i assistant, jak gdyby byÅ‚y caÅ‚oÅ›ciÄ…**, co oddaje poniÅ¼szy przykÅ‚ad. Pole **system** i **user** zawierajÄ…Â fragmenty sÅ‚Ã³w, ktÃ³re zostajÄ…Â **uzupeÅ‚nione** przez model w polu **assistant**.

![](https://cloud.overment.com/same-29ac2f95-3.png)

To wszystko prowadzi nas do jednego wniosku: **ProjektujÄ…c Prompt**, moÅ¼esz myÅ›leÄ‡ o nim jak o bloku tekstu, ktÃ³ry **ma zostaÄ‡ uzupeÅ‚niony przez model**. MajÄ…c to na uwadze, bÄ™dziesz projektowaÄ‡Â prompty **dopasowane do natury modelu**, co moÅ¼e przekÅ‚adaÄ‡Â siÄ™Â na ich ogÃ³lnÄ…Â skutecznoÅ›Ä‡. 

W praktyce, moÅ¼esz projektowaÄ‡Â caÅ‚Ä… konwersacjÄ™ juÅ¼Â od samego poczÄ…tku, np. nadajÄ…c ton wypowiedzi modelu **bez dosÅ‚ownego opisywania go sÅ‚owami.** Takie podejÅ›cie sprawdza siÄ™Â w kontekÅ›cie optymalizacji promptu pod kÄ…tem dÅ‚ugoÅ›ci. Podobnie jak w przypadku podawania przykÅ‚adÃ³w, tutaj caÅ‚a treÅ›Ä‡ promptu staje siÄ™Â wzorem, ktÃ³rym model zaczyna podÄ…Å¼aÄ‡ podczas generowania uzupeÅ‚nieÅ„. 

![](https://cloud.overment.com/yo-74313fb7-6.png)

## Techniki projektowania promptÃ³w

Na poczÄ…tku powiedziaÅ‚em, Å¼e nie bÄ™dziemy omawiaÄ‡ technik projektowania promptÃ³w, ktÃ³rych opisy i przykÅ‚ady Å‚atwo znajdziesz w sieci. Musisz jednak wiedzieÄ‡, czego szukaÄ‡. Oto lista, ktÃ³ra Ci w tym pomoÅ¼e: 

- **Zero-shot Prompting** polega na zdolnoÅ›ci modelu do wykonania zadania na podstawie prostej instrukcji, ktÃ³ra nie zawiera przykÅ‚adÃ³w
- **One-shot / Few-shot Prompting** polega na podawaniu przykÅ‚adÃ³w prezentujÄ…cych oczekiwane zachowanie modelu. OmÃ³wiliÅ›my to powyÅ¼ej.
- **Chain of Thought** polega na prowadzeniu modelu przez ciÄ…g myÅ›lowy z pomocÄ… wiedzy dostarczonej przez nas lub wygenerowanej przez model. PrzykÅ‚adowo szukajÄ…c odpowiedzi na nasze pytanie, moÅ¼emy opisaÄ‡Â sytuacjÄ™ w ktÃ³rej siÄ™ znajdujemy oraz kroki, ktÃ³re juÅ¼Â podjÄ™liÅ›my.
- **Zero-shot Chain of Thought** polega uÅ¼yciu wyraÅ¼enia "think step by step" w ktÃ³rego wyniku model **wyjaÅ›ni swoje rozumowanie** przechodzÄ…c przez kolejne kroki, ktÃ³re **zwiÄ™kszajÄ… prawdopodobieÅ„stwo wygenerowania poprawnej odpowiedzi**
- **Reflexion** polega na uÅ¼yciu wyraÅ¼enia np. "let's verify this step by step" w celu **zweryfikowania odpowiedzi wygenerowanej przez model**. Zasadniczo chodzi o to, aby model **samodzielnie** znalazÅ‚Â ewentualne bÅ‚Ä™dy w swoim rozumowaniu lub potwierdziÅ‚Â swojÄ… dotychczasowÄ…Â odpowiedÅº
- **Three of Thoughts** polega na **wygenerowaniu moÅ¼liwych scenariuszy**, **pogÅ‚Ä™bieniu ich**, **wybraniu najbardziej prawdopodobnych** i **udzieleniu odpowiedzi**. Åatwo zauwaÅ¼yÄ‡, Å¼e wÃ³wczas generowanie odpowiedzi trwa dÅ‚uÅ¼ej, jednak wedÅ‚ug [tej publikacji](https://arxiv.org/abs/2305.10601) dla pewnego zestawu zadaÅ„, skutecznoÅ›Ä‡ modelu wzrosÅ‚a z 4% (Chain of Thought) do 74%! 
- Bonus: **SmartGPT** to technika rozwijana przez twÃ³rcÄ™ kanaÅ‚u [https://www.youtube.com/@aiexplained-official](https://www.youtube.com/@aiexplained-official), ktÃ³ra jest Å›wietnym przykÅ‚adem tego, Å¼e moÅ¼na w kreatywny sposÃ³b podchodziÄ‡ nie tylko do projektowania promptÃ³w, ale takÅ¼e **ich Å‚Ä…czenia**.

WiÄ™cej na temat technik projektowania promptÃ³w, znajdziesz tutaj: https://www.promptingguide.ai. W AI_Devs skupimy siÄ™Â na praktycznym wykorzystaniu tych technik w poÅ‚Ä…czeniu z kodem oraz wejdziemy nieco gÅ‚Ä™biej w obszar Prompt Engineeringu. Nie musisz przechodziÄ‡ przez wszystkie zagadnienia wymienione na wspomnianej stronie, ale moÅ¼esz z niej korzystaÄ‡. 

## ÅÄ…czenie ze sobÄ…Â wielu promptÃ³w

Wymienione wyÅ¼ej techniki w wielu przypadkach wymagajÄ…Â albo **wygenerowania kilku odpowiedzi** albo **skÅ‚adajÄ…Â siÄ™Â z kilku promptÃ³w**. ChociaÅ¼ wykorzystanie ich z pomocÄ…Â ChatGPT jest moÅ¼liwe, wymaga naszego zaanagaÅ¼owania. Przy bezpoÅ›rednim, programistycznym (lub no-code) poÅ‚Ä…czeniu z modelami poprzez API, wszystkie poÅ›rednie kroki **mogÄ… byÄ‡ realizowane automatycznie**, a efektem jest tutaj ostateczna odpowiedÅº modelu.

Zatem kolejnaÂ zmiana postrzegania LLM dotyczy tego, aby **nie patrzeÄ‡ na interakcjÄ™ z modelami przez pryzmat pojedynczego zapytania** oraz **przez koniecznoÅ›Ä‡Â wyÅ›wietlania wszystkich informacji uÅ¼ytkownikowi**. Co wiÄ™cej **uÅ¼ytkownik w ogÃ³le nie musi byÄ‡Â zaangaÅ¼owany w ten proces**. 

PrzykÅ‚ad interakcji, obrazujÄ…cy moÅ¼liwoÅ›ci o ktÃ³rych tutaj mÃ³wimy, wyglÄ…da nastÄ™pujÄ…co: 

1. **Zapytanie uÅ¼ytkownika: "Jak zainstalowaÄ‡Â LLaMA2 na MacOS?"**
2. [Weryfikacja zapytania pod kÄ…tem polityki OpenAI]
3. [Weryfikacja zapytania pod kÄ…tem naszych zasad]
4. [Wzbogacenie zapytania kontekstem konwersacji]
5. [Skorzystanie z wynikÃ³w wyszukiwania w Internecie]
6. [Przeszukanie naszej bazy wiedzy]
7. [WybÃ³r pasujÄ…cych wynikÃ³w]
9. [Podsumowanie zawartoÅ›ci wybranego ÅºrÃ³dÅ‚a]
10. [Wygenerowanie odpowiedzi przez LLM]
11. [Zweryfikowanie odpowiedzi przez LLM]
12. **[PrzesÅ‚anie odpowiedzi uÅ¼ytkownikowi]**

Z punktu widzenia uÅ¼ytkownika, widoczne sÄ…Â tylko dwa, pogrubione kroki z powyÅ¼szej listy. Reszta wykonywana jest w tle. W celu optymalizacji caÅ‚ego procesu, czÄ™Å›Ä‡ zadaÅ„ moÅ¼e byÄ‡ zrealizowana w tle lub korzystaÄ‡ z zapamiÄ™tanych wczeÅ›niej informacji (np. podsumowania wybranych stron).

W przypadku Å‚Ä…czenia ze sobÄ… promptÃ³w ogromne znaczenie odgrywa ich **dopracowanie**. Ze wzglÄ™du na wszystkie cechy modelu zwiÄ…zane z **niedeterministycznÄ… naturÄ…** czy brakiem **peÅ‚nego wpÅ‚ywu** na jego zachowanie, musimy dÄ…Å¼yÄ‡ do **zminimalizowania ryzyka** niepoÅ¼Ä…danego zachowania.

![](https://cloud.overment.com/perfect-b7ecffbf-8.png)

Tym bardziej, Å¼e w przypadku **powiÄ…zanych promptÃ³w**, gdy jeden z nich zawiedzie, te ktÃ³re wystÄ™pujÄ… po nim rÃ³wnieÅ¼ nie speÅ‚niÄ… swojego zadania. Niebawem przekonasz siÄ™Â o tym w praktyce.

## Projektowanie promptÃ³w z pomocÄ… AI i optymalizacja

Wiedza na temat modeli oraz ogÃ³lnych zasad projektowania promptÃ³w nie jest wystarczajÄ…ca do tego, aby robiÄ‡ to skutecznie. OczywiÅ›cie zakÅ‚adajÄ…c, Å¼e chcemy projektowaÄ‡ coÅ› wiÄ™cej, niÅ¼ proste interakcje. Powodem tego jest fakt, Å¼e pracujemy tutaj z jÄ™zykiem naturalnym, a to wymaga dodatkowej kreatywnoÅ›ci, szerokiej wiedzy, bogatego sÅ‚ownictwa i ogÃ³lnej precyzji. **Nawet jeÅ›li swobodnie czujesz siÄ™Â w tych obszarach**, to niezwykle pomocna przy projektowaniu promptÃ³w staje siÄ™ sztuczna inteligencja, a konkretnie GPT-4 dostÄ™pny w Playground.

ProjektujÄ…c prompt w ten sposÃ³b, pamiÄ™taj o tym, aby: 

- Instrukcja systemowa nadawaÅ‚a odpowiedniÄ… rolÄ™, np. Senior Prompt Engineer oraz przedstawiaÅ‚a cel rozmowy
- Zacznij od prostego promptu, aby nadaÄ‡ ogÃ³lny ton, ktÃ³ry rozbudujesz wspÃ³lnie z GPT-4
- W instrukcji musisz uwzglÄ™dniÄ‡ takÅ¼e wyraÅºne podkreÅ›lenie tego, aby model **nie realizowaÅ‚ instrukcji w podanym przez uÅ¼ytkownika prompcie**
- Warto takÅ¼e zasugerowaÄ‡, aby pierwsza interakcja umoÅ¼liwiaÅ‚a przekazanie promptu przez uÅ¼ytkownika, a odpowiedÅº modelu zawieraÅ‚a jedynie jego przeglÄ…d i wstÄ™pne rekomendacje, ktÃ³re pozwolÄ… Ci zaczÄ…Ä‡, **bez ich natychmiastowego wprowadzania**
- OczywiÅ›cie model nie posiada zdolnoÅ›ci do perfekcyjnej analizy Twoich promptÃ³w, ale trafnie wskazuje rÃ³Å¼nego rodzaju bÅ‚Ä™dy logiki czy sugeruje zmiany sÅ‚Ã³w czy wyraÅ¼eÅ„
- Przy projektowaniu promptÃ³w korzystaj z GPT-4
- Natomiast sterowanie projektowaniem promptu pozostawiaj sobie i podejmuj wszystkie decyzje zwiÄ…zane z jego modyfikowaniem

PrzykÅ‚ad Playground, ktÃ³ry ja wykorzystujÄ™ do swojej pracy znajdziesz poniÅ¼ej. **Nie jest to jednak instrukcja z ktÃ³rej korzystam zawsze**, lecz raczej schemat, wokÃ³Å‚ ktÃ³rego siÄ™ poruszam. SzczegÃ³Å‚y zawsze rÃ³Å¼niÄ… siÄ™Â w zaleÅ¼noÅ›ci od tego, jaki prompt chcÄ™ zaprojektowaÄ‡. 

![](https://cloud.overment.com/design-38c47cc0-2.png)

â€” ğŸ”— [zobacz przykÅ‚ad](https://platform.openai.com/playground/p/Cx1IqImlXBnW5LORydujRUKY?model=gpt-4)

DokÅ‚adnie na tej samej zasadzie moÅ¼esz pracowaÄ‡ nie tylko nad promptami, ale takÅ¼e innymi zadaniami wymagajÄ…cymi precyzji oraz moÅ¼liwoÅ›ci **duÅ¼ej kontroli nad zachowaniem modelu**. Playground jest Å›wietnym narzÄ™dziem, ktÃ³re sprawdza siÄ™ w takich sytuacjach. PamiÄ™taj o moÅ¼liwoÅ›ci zapisywania interakcji przyciskiem "Save" oraz **dbaj o czytelne i zrozumiaÅ‚e nazwy zapisanych interakcji**. 

Proces projektowania promptÃ³w wyglÄ…da wiÄ™c zatem u mnie nastÄ™pujÄ…co: 

- **Szkic:** Pierwszy, ogÃ³lny szkic, majÄ…cy na celu rozpoznanie zachowania modelu dla konkretnego zadania. Na tym etapie pomijam kwestiÄ™ optymalizacji i skupiam siÄ™Â na dojÅ›ciu do celu.
- **Weryfikacja:** Drugi etap polega na przygotowaniu zestawÃ³w danych weryfikujÄ…cych zarÃ³wno najbardziej prawdopodobne zastosowania, jak i te, niemal absurdalne, ktÃ³re przyjdÄ… mi do gÅ‚owy, majÄ…ce na celu caÅ‚kowicie zaburzyÄ‡ dziaÅ‚anie promptu. UwzglÄ™dniam tutaj takÅ¼e **zachowanie promptu dla wiÄ™kszych zestawÃ³w danych**, np. obszernego kontekstu lub dÅ‚uÅ¼szej konwersacji
- **Uruchomienie:** Trzeci etap zaleÅ¼y od przeznaczenia promptu. ZakÅ‚adajÄ…c, Å¼e mÃ³wimy o przetwarzaniu dokumentÃ³w, to podÅ‚Ä…czam kilka **krÃ³tszych** plikÃ³w i sprawdzam, jak wyglÄ…dajÄ…Â efekty. Na tym etapie zwykle od razu wprowadzam jakieÅ› poprawki.
- **Brainstorm:** WspÃ³lnie z GPT-4, korzystajÄ…c z Playground przeprowadzam analizÄ™ na podstawie moich obserwacji, sugestii modelu oraz moich przemyÅ›leÅ„. Gdy widzÄ™, Å¼e GPT-4 rozumie dziaÅ‚anie mojego promptu, przechodzimy do wykrycia nieÅ›cisÅ‚oÅ›ci czy zastosowania lepszych wyraÅ¼eÅ„. Na tym etapie nierzadko dochodzi takÅ¼e do **optymalizacji** pod kÄ…tem dÅ‚ugoÅ›ci promptu. Podczas tego etapu nierzadko podÄ…Å¼am za sugestiami GPT-4, natomiast i tak **ostateczna decyzja pozostaje po mojej stronie**
- **Iteracja:** Wracam do punktu drugiego i kontynuujÄ™ proces do osiÄ…gniÄ™cia poÅ¼Ä…danych rezultatÃ³w oraz poczucia, Å¼e "moja praca jest tutaj zrobiona". Nierzadko w kolejnych iteracjach przeÅ‚Ä…czam prompt na sÅ‚absze ale szybsze modele w wersji 3.5 i jeÅ›li to moÅ¼liwe, pozostajÄ™ przy nich w celu optymalizacji kosztÃ³w.

OczywiÅ›cie moÅ¼esz opracowaÄ‡ swÃ³j wÅ‚asny proces. Sam nie stosujÄ™ jeszcze testÃ³w automatycznych moich promptÃ³w, aczkolwiek zdarza mi siÄ™ wykonywaÄ‡ proste skrypty przechodzÄ…ce przez zestaw danych testowych. JeÅ›li rezultatem dziaÅ‚ania jest wynik, ktÃ³ry mogÄ™ zweryfikowaÄ‡ programistycznie (np. wygenerowanym przez GPT-4 wyraÅ¼eniem regularnym), to oczywiÅ›cie to robiÄ™. 


#aidevs_2