---
title: Multimodal CoT Prompting
tags: 
aliases:
---
Zhang  zaproponowali niedawno multimodalne podejście polegające na podpowiadaniu łańcucha myśli. Tradycyjne CoT koncentruje się na modalności językowej. Z kolei multimodalne CoT obejmuje tekst i wizję w dwuetapowej strukturze. Pierwszy etap obejmuje generowanie przesłanek w oparciu o informacje multimodalne. Następnie następuje druga faza, wnioskowanie o odpowiedzi, która wykorzystuje wygenerowane racjonalne informacje.

Multimodalny model CoT (1B) przewyższa GPT-3.5 w teście porównawczym ScienceQA.

![[images/Pasted image 20230924185937.png]]

[Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)
