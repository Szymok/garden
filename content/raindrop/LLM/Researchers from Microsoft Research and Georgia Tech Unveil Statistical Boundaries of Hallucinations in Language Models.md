---
raindrop_id: 691410957
---

# Metadata
Source URL:: https://www.marktechpost.com/2023/12/06/researchers-from-microsoft-research-and-georgia-tech-unveil-statistical-boundaries-of-hallucinations-in-language-models/
Topics:: #halucynacje, #AI, #microsoft

---
# Researchers from Microsoft Research and Georgia Tech Unveil Statistical Boundaries of Hallucinations in Language Models

A key issue that has recently surfaced in Language Models is the high rate at which Language Models (LMs) provide erroneous information, including references to nonexistent article titles. The Merriam-Webster dictionary defines a hallucination as &#39;a plausible but false or misleading response generated by an artificial intelligence algorithm.&#39; In one instance, attorneys who submitted legal research with imagined court cases they thought to be accurate faced a $5,000 penalty. In the medical field, patients&#39; hallucinations may be fatal, and doctors worry about being sued for negligence. Additionally, the media has covered hallucinations extensively, and the President of the United States

## Highlights

## Note

- Language Models (LMs) often provide erroneous information, including references to nonexistent article titles. 
- Hallucinations are defined as plausible but false or misleading responses generated by AI algorithms. 
- Attorneys who submitted legal research with imagined court cases faced penalties. 
- Hallucinations in the medical field can be fatal, and doctors worry about being sued for negligence. 
- The media has extensively covered hallucinations, and the President of the United States issued an Executive Order requesting protections against deceptive results from AI systems. 
- Researchers from Microsoft Research and Georgia Tech present statistical lower bounds on the hallucination rate for learning machines (LMs) that are calibrated fact predictors. 
- Any LM that predicts every string with positive probability will necessarily hallucinate with positive probability. 
- Practitioners are supplementing "pretraining" procedures with "post-training" procedures to lower hallucination rates and calibration. ## Note

**Contents of the Webpage:**

- Language Models (LMs) often provide erroneous information, including references to nonexistent article titles. 
- Hallucinations are defined as plausible but false or misleading responses generated by AI algorithms. 
- Attorneys who submitted legal research with imagined court cases faced penalties. 
- Hallucinations in the medical field can be fatal, and doctors worry about being sued for negligence. 
- The media has extensively covered hallucinations, and the President of the United States issued an Executive Order requesting protections against deceptive results from AI systems. 
- Researchers from Microsoft Research and Georgia Tech present statistical lower bounds on the hallucination rate for learning machines (LMs) that are calibrated fact predictors. 
- Any LM that predicts every string with positive probability will necessarily hallucinate with positive probability. 
- Practitioners are supplementing &quot;pretraining&quot; procedures with &quot;post-training&quot; procedures to lower hallucination rates and calibration. 