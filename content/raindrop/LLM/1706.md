---
raindrop_id: 720579877

---

# Metadata
Source URL:: https://arxiv.org/pdf/1706.03762.pdf


---
# 1706



## Highlights
## Note

Oto podsumowanie części 1 dokumentu:

- **Cel**: Zaproponować nową architekturę modelu, Transformer, która opiera się wyłącznie na mechanizmach uwagi, eliminując rekurencję i konwolucje.
- **Tło**: Większość modeli transdukcji sekwencji opiera się na rekurencyjnych lub konwolucyjnych sieciach neuronowych z uwagą, które są skuteczne, ale wymagają sekwencyjnej obliczeń i są trudne do nauczenia zależności między odległymi pozycjami.
- **Transformer**: Model składa się z dwóch części: kodera i dekodera, złożonych ze stosu identycznych warstw. Każda warstwa zawiera pod-warstwy uwagi wielogłowicowej i punktowo-podłączonej sieci feed-forward. Model jest autoregresywny i generuje wyjście jeden symbol na raz.
- **Uwaga**: Uwaga to funkcja, która mapuje zapytanie i zbiór par klucz-wartość na wyjście, będące ważoną sumą wartości. Uwaga wielogłowicowa to połączenie kilku warstw uwagi, które działają równolegle. Uwaga skalowana iloczynem skalarnym to szczególny przypadek uwagi, który dzieli iloczyn skalarny zapytania i klucza przez pierwiastek z wymiaru zapytania.